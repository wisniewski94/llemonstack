## ⚡️ n8n WorkflowQuick Start

TODO: rewrite this section

The main component of the self-hosted AI starter kit is a docker compose file pre-configured with
network and disk so there isn't much else you need to install. After completing the installation
steps above, follow the steps below to get started.

1. Open <http://localhost:5678/> in your browser to set up n8n. You'll only have to do this once.
   You are NOT creating an account with n8n in the setup here, it is only a local account for your
   instance!
2. Open the included workflow: <http://localhost:5678/workflow/vTN9y2dLXqTiDfPT>
3. Create credentials for every service:

   Ollama URL: http://ollama:11434

   Postgres (through Supabase): use DB, username, and password from .env. IMPORTANT: Host is 'db'
   since that is the name of the service running Supabase

   Qdrant URL: http://qdrant:6333

   Google Drive: Follow
   [this guide from n8n](https://docs.n8n.io/integrations/builtin/credentials/google/). Don't use
   localhost for the redirect URI, just use another domain you have, it will still work!

   Alternatively, you can set up
   [local file triggers](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/).

4. Select **Test workflow** to start running the workflow.
5. If this is the first time you're running the workflow, you may need to wait until Ollama finishes
   downloading Llama3.1. You can inspect the docker console logs to check on the progress.
6. Make sure to toggle the workflow as active and copy the "Production" webhook URL!
7. Open <http://localhost:3000/> in your browser to set up Open WebUI. You'll only have to do this
   once. You are NOT creating an account with Open WebUI in the setup here, it is only a local
   account for your instance!
8. Go to Settings -> Admin Panel -> Functions -> Add Function -> Give name + description then paste
   in the code from `openwebui/n8n_pipe.py`
9. Click on the gear icon and set the n8n_url to the production URL for the webhook you copied in a
   previous step.
10. Toggle the function on and now it will be available in your model dropdown in the top left!

See https://openwebui.com/functions?query=n8n for more n8n functions.

To open n8n at any time, visit <http://localhost:5678/> in your browser. To open Open WebUI at any
time, visit <http://localhost:3000/>.

With your n8n instance, you'll have access to over 400 integrations and a suite of basic and
advanced AI nodes such as
[AI Agent](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/),
[Text classifier](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/),
and
[Information Extractor](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/)
nodes. To keep everything local, just remember to use the Ollama node for your language model and
Qdrant as your vector store.

> [!NOTE]
> This starter kit is designed to help you get started with self-hosted AI workflows. While it's not
> fully optimized for production environments, it combines robust components that work well together
> for proof-of-concept projects. You can customize it to meet your specific needs
